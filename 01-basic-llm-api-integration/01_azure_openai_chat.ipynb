{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9496d0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧑 You: Tell me a joke\n",
      "🤖 Azure LLM: Why did the scarecrow win an award? \n",
      "\n",
      "Because he was outstanding in his field!\n",
      "\n",
      "🧑 You: What is the capital of France?\n",
      "🤖 Azure LLM: The capital of France is Paris.\n",
      "\n",
      "🧑 You: Explain quantum computing in one sentence.\n",
      "🤖 Azure LLM: Quantum computing is a type of computation that utilizes the principles of quantum mechanics to process information using quantum bits (qubits), which can exist in multiple states simultaneously, allowing for vastly greater computing power for certain complex problems compared to classical computers.\n",
      "\n",
      "🧑 You: Translate 'Good morning' into Japanese.\n",
      "🤖 Azure LLM: 'Good morning' in Japanese is \"おはようございます\" (ohayou gozaimasu).\n",
      "\n",
      "🧑 You: Give me one fun fact about space.\n",
      "🤖 Azure LLM: One fun fact about space is that there is a giant cloud of alcohol in the universe; it's called Sagittarius B2 and contains enough ethyl alcohol to make around 400 trillion trillion pints of beer!\n",
      "\n",
      "🧑 You: What was the first question I asked you?\n",
      "🤖 Azure LLM: The first question you asked was, \"Tell me a joke.\"\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "# Azure client setup\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "# Start conversation with system role\n",
    "conversation = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "# Prompt 1 - asking for a joke\n",
    "prompt1 = \"Tell me a joke\"\n",
    "conversation.append({\"role\": \"user\", \"content\": prompt1})\n",
    "response1 = client.chat.completions.create(\n",
    "    model=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "    messages=conversation\n",
    ")\n",
    "reply1 = response1.choices[0].message.content.strip()\n",
    "conversation.append({\"role\": \"assistant\", \"content\": reply1})\n",
    "print(f\"\\n🧑 You: {prompt1}\")\n",
    "print(f\"🤖 Azure LLM: {reply1}\")\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "# Prompt 2 - asking for the capital of France\n",
    "prompt2 = \"What is the capital of France?\"\n",
    "conversation.append({\"role\": \"user\", \"content\": prompt2})\n",
    "response2 = client.chat.completions.create(\n",
    "    model=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "    messages=conversation\n",
    ")\n",
    "reply2 = response2.choices[0].message.content.strip()\n",
    "conversation.append({\"role\": \"assistant\", \"content\": reply2})\n",
    "print(f\"\\n🧑 You: {prompt2}\")\n",
    "print(f\"🤖 Azure LLM: {reply2}\")\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "# Prompt 3 - asking for an explanation of quantum computing\n",
    "prompt3 = \"Explain quantum computing in one sentence.\"\n",
    "conversation.append({\"role\": \"user\", \"content\": prompt3})\n",
    "response3 = client.chat.completions.create(\n",
    "    model=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "    messages=conversation\n",
    ")\n",
    "reply3 = response3.choices[0].message.content.strip()\n",
    "conversation.append({\"role\": \"assistant\", \"content\": reply3})\n",
    "print(f\"\\n🧑 You: {prompt3}\")\n",
    "print(f\"🤖 Azure LLM: {reply3}\")\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "# Prompt 4 - asking for a translation\n",
    "prompt4 = \"Translate 'Good morning' into Japanese.\"\n",
    "conversation.append({\"role\": \"user\", \"content\": prompt4})\n",
    "response4 = client.chat.completions.create(\n",
    "    model=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "    messages=conversation\n",
    ")\n",
    "reply4 = response4.choices[0].message.content.strip()\n",
    "conversation.append({\"role\": \"assistant\", \"content\": reply4})\n",
    "print(f\"\\n🧑 You: {prompt4}\")\n",
    "print(f\"🤖 Azure LLM: {reply4}\")\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "# Prompt 5 - asking for a fun fact about space\n",
    "prompt5 = \"Give me one fun fact about space.\"\n",
    "conversation.append({\"role\": \"user\", \"content\": prompt5})\n",
    "response5 = client.chat.completions.create(\n",
    "    model=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "    messages=conversation\n",
    ")\n",
    "reply5 = response5.choices[0].message.content.strip()\n",
    "conversation.append({\"role\": \"assistant\", \"content\": reply5})\n",
    "print(f\"\\n🧑 You: {prompt5}\")\n",
    "print(f\"🤖 Azure LLM: {reply5}\")\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "# Prompt 6 - asking about the first question\n",
    "prompt6 = \"What was the first question I asked you?\"\n",
    "conversation.append({\"role\": \"user\", \"content\": prompt6})\n",
    "response6 = client.chat.completions.create(\n",
    "    model=AZURE_OPENAI_DEPLOYMENT_NAME,\n",
    "    messages=conversation\n",
    ")\n",
    "reply6 = response6.choices[0].message.content.strip()\n",
    "conversation.append({\"role\": \"assistant\", \"content\": reply6})\n",
    "print(f\"\\n🧑 You: {prompt6}\")\n",
    "print(f\"🤖 Azure LLM: {reply6}\")\n",
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1ffad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 4\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "class LLM: \n",
    "    def __init__(self, model: str, api_key: str, base_url: str) -> None:\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API key is missing.\")\n",
    "        if not base_url:\n",
    "            raise ValueError(\"Base URL is missing.\")\n",
    "\n",
    "        self.model = model\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url.rstrip(\"/\")  # Only call rstrip if not None\n",
    "        self.headers = {\"api-key\": self.api_key}\n",
    "        self.endpoint = f\"{self.base_url}/openai/deployments/{self.model}/chat/completions?api-version=2023-07-01-preview\"\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        payload = {\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.2\n",
    "        }\n",
    "        response = requests.post(self.endpoint, headers=self.headers, json=payload)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            print(f\"❌ Error: {response.status_code} - {response.text}\")\n",
    "            return f\"Error: {response.status_code}\"\n",
    "\n",
    "# 🔐 Get from environment or hardcode directly for testing\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "base_url = os.getenv(\"AZURE_OPENAI_ENDPOINT\") \n",
    "model = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "# ✅ Create and use\n",
    "llm = LLM(\n",
    "    model=model,\n",
    "    api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "\n",
    "output = llm.generate(\"What is 2+2? Answer only numbers\")\n",
    "print(\"Response:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a1046a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Azure Chatbot Ready! Type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧑 You: Hello\n",
      "🤖 Azure LLM: Hello! How can I assist you today?\n",
      "\n",
      "🧑 You: Tell me one joke\n",
      "🤖 Azure LLM: Sure! Here you go:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "🧑 You: Tell me one joke\n",
      "🤖 Azure LLM: Of course! Here’s another one for you:\n",
      "\n",
      "Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field!\n",
      "\n",
      "🧑 You: tell me one more\n",
      "🤖 Azure LLM: Sure! Here's another joke:\n",
      "\n",
      "What do you call fake spaghetti?\n",
      "\n",
      "An impasta!\n",
      "\n",
      "🧑 You: Okay bye\n",
      "🤖 Azure LLM: Goodbye! If you ever want to chat again, feel free to reach out. Have a great day!\n",
      "\n",
      "👋 Bye!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Azure client setup\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "# Initial conversation history\n",
    "conversation = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "def get_response(prompt: str) -> str:\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt})\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=conversation\n",
    "    )\n",
    "    reply = response.choices[0].message.content.strip()\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    return reply\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🧠 Azure Chatbot Ready! Type 'exit' to quit.\\n\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # No prefix input\n",
    "            user_input = input().strip()\n",
    "\n",
    "            # Exit check before echo or API call\n",
    "            if user_input.lower() == \"exit\":\n",
    "                print(\"👋 Bye!\")\n",
    "                break\n",
    "\n",
    "            if not user_input:\n",
    "                continue\n",
    "\n",
    "            print(f\"🧑 You: {user_input}\")\n",
    "            response = get_response(user_input)\n",
    "            print(f\"🤖 Azure LLM: {response}\\n\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n👋 Interrupted. Exiting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439656f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
